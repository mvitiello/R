{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Unit Tests to R Packages\n",
    "\n",
    "In the final chapter, you will learn how to add tests to your package to ensure your code runs as expected if the package is updated or changes. You will look at how to test functions to ensure they produce expected values, and also how to test for other aspects of functionality such as expected errors. Once you've written tests for your functions, you'll finally learn how to run your tests and what to do in the case of a failing test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the test structure\n",
    "You can set up a test framework in a package using the function use_testthat().\n",
    "\n",
    "This will create a tests directory that contains:\n",
    "\n",
    "A script testthat.R.\n",
    "\n",
    "A directory testthat.\n",
    "\n",
    "You save your tests in the tests/testthat/ directory in files with filenames beginning with test-. So, for example, the simutils package has tests named:\n",
    "\n",
    "test-na_counter.R\n",
    "\n",
    "test-sample_from_data.R\n",
    "\n",
    "There are no other strict rules governing the filenames, but you may find it easier to keep track of which functions you are testing if you name your tests after the functions like in the examples above. Alternatively, you can name your tests after areas of package functionality, for example, putting tests for multiple summary functions in test-summaries.R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run\n",
    "# Set up the test framework\n",
    "use_testthat(\"datasummary\")\n",
    "\n",
    "# Look at the contents of the package root directory\n",
    "dir(\"datasummary\")\n",
    "\n",
    "# Look at the contents of the new folder which has been created \n",
    "dir(\"datasummary/tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing an individual test\n",
    "You create individual tests within your test files using functions named with the pattern expect_*. To make your code easier to read, you may want to create the object to be tested (and/or the expected value, if there is one) before you call the expect_* function.\n",
    "\n",
    "Here is one of the tests from the simutils package.\n",
    "\n",
    "air_expected <- c(Ozone = 37, Solar.R = 7, Wind = 0, Temp = 0, Month = 0, Day = 0)\n",
    "\n",
    "expect_equal(na_counter(airquality), air_expected)\n",
    "\n",
    "The expect_* functions differ in the number of parameters, but the first parameter is always the object being tested.\n",
    "\n",
    "When you run your tests, you might notice that there is no output. You will only see an output message if the test has failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not run\n",
    "# Create a summary of the iris dataset using your data_summary() function\n",
    "iris_summary <- data_summary(iris)\n",
    "\n",
    "# Count how many rows are returned\n",
    "summary_rows <- nrow(iris_summary)\n",
    "\n",
    "# Use expect_equal to test that calling data_summary() on iris returns 4 rows\n",
    "expect_equal(summary_rows, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for equality\n",
    "You can use expect_equal(), expect_equivalent() and expect_identical() in order to test whether the output of a function is as expected.\n",
    "\n",
    "These three functions all have slightly different functionality:\n",
    "\n",
    "expect_identical() checks that the values, attributes, and type of both objects are the same.\n",
    "\n",
    "expect_equal() checks that the values, and attributes of both objects are the same. You can adjust how strict expect_equal() is by adjusting the tolerance parameter.\n",
    "\n",
    "expect_equivalent() checks that the values, of both objects are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: result$sd not equal to c(0.23, 0.27).\nLengths differ: 1 is not 2\n",
     "output_type": "error",
     "traceback": [
      "Error: result$sd not equal to c(0.23, 0.27).\nLengths differ: 1 is not 2\nTraceback:\n",
      "1. expect_equal(result$sd, c(0.23, 0.27), tolerance = 0.2)",
      "2. expect(comp$equal, sprintf(\"%s not equal to %s.\\n%s\", act$lab, \n .     exp$lab, comp$message), info = info)",
      "3. exp_signal(exp)",
      "4. withRestarts(if (expectation_broken(exp)) {\n .     stop(exp)\n . } else {\n .     signalCondition(exp)\n . }, continue_test = function(e) NULL)",
      "5. withOneRestart(expr, restarts[[1L]])",
      "6. doWithOneRestart(return(expr), restart)"
     ]
    }
   ],
   "source": [
    "# install.packages(\"testthat\")\n",
    "library(testthat)\n",
    "\n",
    "numeric_summary <- function(x, na.rm){\n",
    "    \n",
    "    if(!is.numeric(x)){\n",
    "        stop(\"data must be numeric\")\n",
    "    }\n",
    "    \n",
    "    if(!na.rm & any(is.na(x))){\n",
    "    warning(\"Data contains NA values!\")\n",
    "  }\n",
    "    \n",
    "    data.frame( min = min(x, na.rm = na.rm),\n",
    "                median = median(x, na.rm = na.rm),\n",
    "                sd = sd(x, na.rm = na.rm),\n",
    "                max = max(x, na.rm = na.rm))\n",
    "}\n",
    "\n",
    "\n",
    "data = runif(20)\n",
    "\n",
    "result <- numeric_summary(data, TRUE)\n",
    "\n",
    "# Update this test so it passes\n",
    "expect_equal(result$sd, c(0.23, 0.27), tolerance = 0.2)\n",
    "\n",
    "expected_result <- list(\n",
    "    min = c(0.01L, 0.02L),\n",
    "    median = c(0.2L, 0.5L),\n",
    "    sd = c(0.2, 0.4),\n",
    "    max = c(0.8L, 0.9L)\n",
    ")\n",
    "\n",
    "# Write a passing test that compares expected_result to result\n",
    "expect_equivalent(result, expected_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing errors\n",
    "You can use expect_error() to test if running a function returns an error. If the function returns an error, the test will pass, otherwise, it will fail. You can optionally define the error message that should be returned to ensure that you are testing for the correct error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>min</th><th scope=col>median</th><th scope=col>sd</th><th scope=col>max</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1      </td><td>5.5    </td><td>3.02765</td><td>10     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " min & median & sd & max\\\\\n",
       "\\hline\n",
       "\t 1       & 5.5     & 3.02765 & 10     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| min | median | sd | max |\n",
       "|---|---|---|---|\n",
       "| 1       | 5.5     | 3.02765 | 10      |\n",
       "\n"
      ],
      "text/plain": [
       "  min median sd      max\n",
       "1 1   5.5    3.02765 10 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a vector containing the numbers 1 through 10\n",
    "my_vector <- 1:10\n",
    "\n",
    "# Look at what happens when we apply this vector as an argument to data_summary()\n",
    "numeric_summary(my_vector, TRUE)\n",
    "\n",
    "# Test if running data_summary() on this vector returns an error\n",
    "expect_error(data_summary(my_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing warnings\n",
    "You can use expect_warning() to test if the output of a function also returns a warning. If the function returns a warning, the test will pass, otherwise, it will fail. You can optionally define the warning message that should be returned to ensure that you are testing for the correct warning.\n",
    "\n",
    "Your data_summary() function has been updated to issue a warning if na.rm is set to FALSE and if the data contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in numeric_summary(my_vector_NA, FALSE):\n",
      "\"Data contains NA values!\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>min</th><th scope=col>median</th><th scope=col>sd</th><th scope=col>max</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>NA</td><td>NA</td><td>NA</td><td>NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " min & median & sd & max\\\\\n",
       "\\hline\n",
       "\t NA & NA & NA & NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| min | median | sd | max |\n",
       "|---|---|---|---|\n",
       "| NA | NA | NA | NA |\n",
       "\n"
      ],
      "text/plain": [
       "  min median sd max\n",
       "1 NA  NA     NA NA "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_vector_NA = c(1,4,3,6,7, NA, 9, 10)\n",
    "# Run data_summary on the airquality dataset with na.rm set to FALSE\n",
    "numeric_summary(my_vector_NA, FALSE)\n",
    "\n",
    "# Use expect_warning to formally test this\n",
    "expect_warning(numeric_summary(my_vector_NA, na.rm = FALSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing non-exported functions\n",
    "As only exported functions are loaded when tests are being run, you can test non-exported functions by referring to them using the package name, followed by three colons, and then the function name (datasummary:::numeric_summary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run \n",
    "# Expected result\n",
    "expected <- data.frame(min = 14L, median = 19L, sd = 3.65148371670111, max = 24L)\n",
    "\n",
    "# Create variable result by calling numeric summary on the temp column of the weather dataset\n",
    "result <- datasummary:::numeric_summary(weather$Temp, na.rm = TRUE)\n",
    "\n",
    "# Test that the value returned matches the expected value\n",
    "expect_equivalent(result, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping tests\n",
    "So far, you've been using expect_*() functions to create individual tests. To run tests in packages you need to group these individual tests together. You do this using a function test_that(). You can use this to group together expectations that test specific functionality.\n",
    "\n",
    "You can use context() to collect these groups together. You usually have one context per file. An advantage of doing this is that it makes it easier to work out where failing tests are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run \n",
    "\n",
    "# Use context() and test_that() to group the tests below together\n",
    "\n",
    "context(\"Test data_summary()\")\n",
    "\n",
    "test_that(\"data_summary() handles errors correctly\", {\n",
    "\n",
    "  # Create a vector\n",
    "  my_vector <- 1:10\n",
    "\n",
    "  # Use expect_error()\n",
    "  expect_error(data_summary(my_vector))\n",
    "\n",
    "  # Use expect_warning()\n",
    "  expect_warning(data_summary(airquality, na.rm = FALSE))\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing unit tests\n",
    "With your tests scripts saved in the package structure you can always easily re-run your tests using the test() function in devtools. This function looks for all tests located in the tests/testhat or inst/tests directory with filenames beginning with test- and ending in .R, and executes each of them. As with the other devtools functions, you supply the path to the package as the first argument to the test() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run \n",
    "# Run the tests on the datasummary package\n",
    "test(\"datasummary\")\n",
    "\n",
    "# Final output \n",
    "== Results =====================================================================\n",
    "Duration: 0.1 s\n",
    "\n",
    "OK:       7\n",
    "Failed:   1\n",
    "Warnings: 1\n",
    "Skipped:  0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
